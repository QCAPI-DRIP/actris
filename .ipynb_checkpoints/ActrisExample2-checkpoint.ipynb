{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## This script read metadata from netCDF file, convert it into rdf triple store.\n",
    "## Xiaofeng Liao\n",
    "## 25 Feb, 2020\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-102418f64239>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-102418f64239>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install netCDF4\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install netCDF4\n",
    "pip install rdflib\n",
    "pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import netCDF4\n",
    "import rdflib\n",
    "from rdflib import Graph, Literal, BNode, Namespace, RDF, URIRef, plugin, ConjunctiveGraph\n",
    "from rdflib.namespace import DC, FOAF, RDFS\n",
    "from rdflib.store import Store\n",
    "#from rdflib.namespace import RDFS\n",
    "from rdflib import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/admin/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument filter_absorption_photometer and variable aerosol_absorption_coefficient:\n",
      "Filename: DK0025G.20030101000000.20170713083448.filter_absorption_photometer.aerosol_absorption_coefficient.aerosol.12y.1h.US08L_Magee_AE16_SUM.US08L_aethalometer.lev2.nc\n",
      "\n",
      "\n",
      "Conventions: CF-1.7, ACDD-1.3\n",
      "featureType: timeSeries\n",
      "title: Ground based in situ observations of aerosol_absorption_coefficient at Summit (DK0025G) using filter_absorption_photometer\n",
      "keywords: NOAA-ESRL, DK0025G, aerosol_absorption_coefficient, Summit, aerosol, GAW-WDCA, volume_absorption_coefficient_in_air_due_to_dried_aerosol_particles\n",
      "id: DK0025G.20030101000000.20170713083448.filter_absorption_photometer.aerosol_absorption_coefficient.aerosol.12y.1h.US08L_Magee_AE16_SUM.US08L_aethalometer.lev2.nc\n",
      "naming_authority: no.nilu.ebas\n",
      "project: NOAA-ESRL, GAW-WDCA\n",
      "acknowledgement: Request acknowledgement details from data originator\n",
      "license: NOAA-ESRL: , GAW-WDCA: \n",
      "summary: Ground based in situ observations of aerosol_absorption_coefficient at Summit (DK0025G) using filter_absorption_photometer. These measurements are gathered as a part of the following projects NOAA-ESRL, GAW-WDCA and they are stored in the EBAS database (http://ebas.nilu.no/). Parameters measured are: aerosol_absorption_coefficient in aerosol (volume_absorption_coefficient_in_air_due_to_dried_aerosol_particles)\n",
      "source: surface observation\n",
      "institution: US08L, National Science Foundation, NSF/OPP, Office of Polar Programs, NSF/OPP, 4201 Wilson Boulevard, , 22230, Arlington VA, USA\n",
      "processing_level: 2: final (Physical parameters, aggregated (if needed), info on variability recommended, quality assured by human inspection. This is the typical EBAS data level meant for dissemination and long term preservation.)\n",
      "date_created: 2017-07-13T08:34:48 UTC\n",
      "date_metadata_modified: 2017-07-13T08:34:48 UTC\n",
      "creator_name: Mike Bergin, John Ogren\n",
      "creator_type: person\n",
      "creator_email: , John.A.Ogren@noaa.gov\n",
      "creator_institution: , \"National Oceanic and Atmospheric Administration/Earth System Research Laboratory/Global Monitoring Division, NOAA/ESRL/GMD\"\n",
      "contributor_name: Mike Bergin\n",
      "contributor_role: data submitter\n",
      "publisher_type: institution\n",
      "publisher_name: NILU - Norwegian Institute for Air Research, ATMOS, EBAS\n",
      "publisher_institution: NILU - Norwegian Institute for Air Research, ATMOS, EBAS\n",
      "publisher_email: ebas@nilu.no\n",
      "publisher_url: http://ebas.nilu.no/\n",
      "geospatial_bounds: POINT Z (72.5800018311 -38.4799995422 3238.0)\n",
      "geospatial_bounds_crs: EPSG:4979\n",
      "geospatial_lat_min: 72.5800018311\n",
      "geospatial_lat_max: 72.5800018311\n",
      "geospatial_lon_min: -38.4799995422\n",
      "geospatial_lon_max: -38.4799995422\n",
      "geospatial_vertical_min: 3238.0\n",
      "geospatial_vertical_max: 3238.0\n",
      "geospatial_vertical_positive: up\n",
      "time_coverage_start: 2003-01-01T00:00:00 UTC\n",
      "time_coverage_end: 2015-01-01T00:00:00 UTC\n",
      "time_coverage_duration: P0012-00-00T00:00:00\n",
      "time_coverage_resolution: P0000-00-00T01:00:00\n",
      "timezone: UTC\n",
      "ebas_metadata: {\n",
      "    \"Data definition\": \"EBAS_1.1\", \n",
      "    \"Set type code\": \"TU\", \n",
      "    \"Timezone\": \"UTC\", \n",
      "    \"File name\": \"DK0025G.20030101000000.20170713083448.filter_absorption_photometer.aerosol_absorption_coefficient.aerosol.12y.1h.US08L_Magee_AE16_SUM.US08L_aethalometer.lev2.nc\", \n",
      "    \"File creation\": \"20190513125103\", \n",
      "    \"Startdate\": \"20030101000000\", \n",
      "    \"Revision date\": \"20170713083448\", \n",
      "    \"Version\": \"1\", \n",
      "    \"Data level\": \"2\", \n",
      "    \"Period code\": \"12y\", \n",
      "    \"Resolution code\": \"1h\", \n",
      "    \"Sample duration\": \"1h\", \n",
      "    \"Orig. time res.\": \"1mn\", \n",
      "    \"Station code\": \"DK0025G\", \n",
      "    \"Platform code\": \"DK0025S\", \n",
      "    \"Station name\": \"Summit\", \n",
      "    \"Station WDCA-ID\": \"GAWADK__SUM\", \n",
      "    \"Station GAW-ID\": \"SUM\", \n",
      "    \"Station GAW-Name\": \"Summit\", \n",
      "    \"Station land use\": \"Snowfield\", \n",
      "    \"Station setting\": \"Polar\", \n",
      "    \"Station GAW type\": \"G\", \n",
      "    \"Station WMO region\": \"6\", \n",
      "    \"Station latitude\": \"72.5800018311\", \n",
      "    \"Station longitude\": \"-38.4799995422\", \n",
      "    \"Station altitude\": \"3238.0 m\", \n",
      "    \"Regime\": \"IMG\", \n",
      "    \"Component\": \"aerosol_absorption_coefficient\", \n",
      "    \"Unit\": \"1/Mm\", \n",
      "    \"Matrix\": \"aerosol\", \n",
      "    \"Laboratory code\": \"US08L\", \n",
      "    \"Instrument type\": \"filter_absorption_photometer\", \n",
      "    \"Instrument name\": \"Magee_AE16_SUM\", \n",
      "    \"Instrument manufacturer\": \"Magee\", \n",
      "    \"Instrument model\": \"AE16\", \n",
      "    \"Instrument serial number\": \"175\", \n",
      "    \"Method ref\": \"US08L_aethalometer\", \n",
      "    \"Standard method\": \"SOP=GAW227\", \n",
      "    \"Inlet type\": \"Downward-facing tube\", \n",
      "    \"Humidity/temperature control\": \"None\", \n",
      "    \"Volume std. temperature\": \"273.15 K\", \n",
      "    \"Volume std. pressure\": \"1013.25 hPa\", \n",
      "    \"Detection limit\": \"0.047429 1/Mm\", \n",
      "    \"Detection limit expl.\": \"Determined by instrument noise characteristics, no detection limit flag used\", \n",
      "    \"Measurement uncertainty\": \"25.0 %\", \n",
      "    \"Measurement uncertainty expl.\": \"Intercomparisons with thermo-optical analysis have shown an uncertainty of a faktor 2 regardless specific absorption cross-section assumed.\", \n",
      "    \"Zero/negative values code\": \"Zero/negative possible\", \n",
      "    \"Zero/negative values\": \"Zero and neg. values may appear due to statistical variations at very low concentrations\", \n",
      "    \"Organization\": \"US08L, \\\"National Science Foundation, NSF/OPP, Office of Polar Programs\\\", NSF/OPP, , \\\"4201 Wilson Boulevard, , 22230, Arlington VA, USA\\\", , , , \", \n",
      "    \"Frameworks\": \"GAW-WDCA NOAA-ESRL\", \n",
      "    \"Originator\": \"Ogren, John, John.A.Ogren@noaa.gov, National Oceanic and Atmospheric Administration/Earth System Research Laboratory/Global Monitoring Division, NOAA/ESRL/GMD, , 325 Broadway, , 80305, \\\"Boulder, CO\\\", USA\", \n",
      "    \"Submitter\": \"Bergin, Mike, , , , , , , , , \", \n",
      "    \"Acknowledgement\": \"Request acknowledgement details from data originator\", \n",
      "    \"Comment\": \"calibration factor / mean ratio=1.0; sample_spot_area=5e-5 m2; Sigma=16.0. Raw EBC data as reported by instrument, which is a factor of 2.0 higher than the value calculated from the reported intensities. EBC should be calculated from intensities.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########\n",
    "## DMPS ##\n",
    "##########\n",
    "\n",
    "ae = Namespace(\"http://actrisexample.eu/ns/\")\n",
    "\n",
    "vocab = dict()\n",
    "vocab[None] = {'uri': URIRef('http://actrisexample.eu/ns/NULL'), 'label': 'None'}\n",
    "vocab['NULL'] = {'uri': URIRef('http://actrisexample.eu/ns/NULL'), 'label': 'NULL'}\n",
    "\n",
    "store = plugin.get('IOMemory', Store)()\n",
    "\n",
    "g = ConjunctiveGraph(store)\n",
    "g.bind('envri', 'http://envri.eu/ns/')\n",
    "g.bind('ae','http://actrisexample.eu/ns/')\n",
    "g.bind('dcterms', 'http://purl.org/dc/terms/')\n",
    "#g.bind('foaf', 'http://xmlns.com/foaf/0.1/')\n",
    "g.bind('skos', 'http://www.w3.org/2004/02/skos/core#')\n",
    "\n",
    "# with open('vocab.yaml', 'r') as f:\n",
    "#     for key, value in yaml.safe_load(f).items():\n",
    "#         v = dict()\n",
    "#         v['uri'] = URIRef(value['uri'])\n",
    "#         v['label'] = value['label']\n",
    "#         vocab[key] = v\n",
    "#         g.add((vocab[key]['uri'], RDFS.label, Literal(vocab[key]['label'])))\n",
    "\n",
    "#print('Instrument DMPS and variable particle number size distribution:\\nFilename: NO0058G.20070218000000.20181205100800.dmps.particle_number_size_distribution.aerosol.3y.1h.NO01L_DMPS_Troll1.NO01L_dmps_DMPS_Troll1.lev2.nc\\n\\n')\n",
    "print('Instrument filter_absorption_photometer and variable aerosol_absorption_coefficient:\\nFilename: DK0025G.20030101000000.20170713083448.filter_absorption_photometer.aerosol_absorption_coefficient.aerosol.12y.1h.US08L_Magee_AE16_SUM.US08L_aethalometer.lev2.nc\\n\\n')\n",
    "\n",
    "# use the netcdf4 lib to extract netcdf via the opendap protocol\n",
    "#dataset = netCDF4.Dataset('NO0058G.20070218000000.20181205100800.dmps.particle_number_size_distribution.aerosol.3y.1h.NO01L_DMPS_Troll1.NO01L_dmps_DMPS_Troll1.lev2.nc','r','NETCDF4')\n",
    "dataset = netCDF4.Dataset('DK0025G.20030101000000.20170713083448.filter_absorption_photometer.aerosol_absorption_coefficient.aerosol.12y.1h.US08L_Magee_AE16_SUM.US08L_aethalometer.lev2.nc','r','NETCDF4')\n",
    "# fetch global attributes from netcdf on the thredds server\n",
    "global_attr = dataset.ncattrs()\n",
    "\n",
    "\n",
    "ds = BNode()\n",
    "dmps = BNode()\n",
    "particle_number_size_distribution = BNode()\n",
    "aerosol = BNode()\n",
    "g.add( (aerosol, RDFS.label, Literal('aerosol')))\n",
    "\n",
    "\n",
    "for attr in global_attr:\n",
    "    print('{0}: {1}'.format(attr, dataset.getncattr(attr)))\n",
    "#     if attr=='Conventions':\n",
    "#         g.add( (ds, ae.Conventions, Literal(dataset.getncattr(attr))) )\n",
    "#     elif attr=='featureType':\n",
    "#         g.add( ( ds, ae.featureType, Literal(dataset.getncattr(attr))))#timeSeries\n",
    "#     elif attr=='title':\n",
    "#         g.add( ( ds, ae.title, Literal(dataset.getncattr(attr))))#'Ground based in situ observations of particle_number_size_distribution at Troll (NO0058G) using dmps')))\n",
    "#     elif attr=='keywords':\n",
    "#         g.add( ( ds, ae.keywords, Literal(dataset.getncattr(attr))))#'Troll, NO0058G, number_concentration_of_positive_ions_in_dry_aerosol_particles_in_air, aerosol, GAW-WDCA, particle_number_size_distribution, NILU\n",
    "#     elif attr=='id':\n",
    "#         g.add( ( ds, ae.id, Literal(dataset.getncattr(attr))))#NO0058G.20070218000000.20181205100800.dmps.particle_number_size_distribution.aerosol.3y.1h.NO01L_DMPS_Troll1.NO01L_dmps_DMPS_Troll1.lev2.nc\n",
    "#     elif attr=='naming_authority':\n",
    "#         g.add( ( ds, ae.naming_authority, Literal(dataset.getncattr(attr))))# no.nilu.ebas\n",
    "#     elif attr=='project':\n",
    "#         g.add( ( ds, ae.project, Literal(dataset.getncattr(attr))))#GAW-WDCA, NILU\n",
    "#     elif attr=='license':\n",
    "#         g.add( ( ds, ae.license, Literal(dataset.getncattr(attr))))#GAW-WDCA: , NILU: Public open access. We encourage contacting data originators if substatial use of individual time series is planned (fair use data policy).\n",
    "#     elif attr==\"summary\":\n",
    "#         print(attr)\n",
    "#         print(dataset.getncattr(attr))\n",
    "#         sentence = dataset.getncattr(attr)\n",
    "#         tokens = nltk.word_tokenize(sentence)\n",
    "#         tagged = nltk.pos_tag(tokens)\n",
    "#         print(\"\\n###########\\n\")\n",
    "#         print(\"pos tagging:\")\n",
    "#         #print(tagged)\n",
    "#         print(\"\\n###########\\n\")\n",
    "#         #print(\"entities:\")\n",
    "#         entities = nltk.chunk.ne_chunk(tagged)\n",
    "#         #print(entities)\n",
    "#     # Add triples using store's add method.\n",
    "#         g.add( (particle_number_size_distribution, RDFS.label, Literal(\"particle_number_size_distribution\")))\n",
    "#         g.add( (dmps, ae.measures, particle_number_size_distribution ) )\n",
    "#         g.add( (particle_number_size_distribution, ae.belongsto, aerosol))\n",
    "#     #g.add( (donna, FOAF.nick, Literal(\"donna\", lang=\"foo\")) )\n",
    "#     #g.add( (donna, FOAF.name, Literal(\"Donna Fales\")) )\n",
    "#     #g.add( (donna, FOAF.mbox, URIRef(\"mailto:donna@example.org\")) )\n",
    "#     #summary: Ground based in situ observations of particle_number_size_distribution at Troll (NO0058G) using dmps. These measurements are gathered as a part of the following projects GAW-WDCA, NILU and they are stored in the EBAS database (http://ebas.nilu.no/). Parameters measured are: particle_number_size_distribution in aerosol (number_concentration_of_positive_ions_in_dry_aerosol_particles_in_air)\n",
    "#     elif attr=='source':\n",
    "#         g.add( ( ds, ae.source, Literal(dataset.getncattr(attr))))#surface observation\n",
    "#     elif attr== 'institution': \n",
    "#         g.add( ( ds, ae.institution, Literal(dataset.getncattr(attr))))#NO01L, Norwegian Institute for Air Research, NILU, Atmosphere and Climate Department, Instituttveien 18, 2007, Kjeller, Norway\n",
    "#     elif attr=='processing_level':\n",
    "#         g.add( ( ds, ae.processing_level, Literal(dataset.getncattr(attr))))#2: final (Physical parameters, aggregated (if needed), info on variability recommended, quality assured by human inspection. This is the typical EBAS data level meant for dissemination and long term preservation.)\n",
    "#     elif attr== 'date_created': \n",
    "#         g.add( ( ds, ae.date_created, Literal(dataset.getncattr(attr))))#2018-12-05T10:08:00 UTC\n",
    "#     elif attr =='date_metadata_modified':\n",
    "#         g.add( ( ds, ae.date_metadata_modified, Literal(dataset.getncattr(attr))))#2018-12-05T10:08:00 UTC\n",
    "#     elif attr =='creator_name':\n",
    "#         g.add( ( ds, ae.creator_name, Literal(dataset.getncattr(attr))))#Chris Lunder\n",
    "#     elif attr =='creator_type': \n",
    "#         g.add( ( ds, ae.creator_type, Literal(dataset.getncattr(attr))))#person\n",
    "#     elif attr =='creator_email': \n",
    "#         g.add( ( ds, FOAF.mbox, URIRef(dataset.getncattr(attr))))#crl@nilu.no\n",
    "#     elif attr =='creator_institution': \n",
    "#         g.add( ( dmps, ae.creator_institution, Literal(dataset.getncattr(attr))))#\"Norwegian Institute for Air Research, Atmosphere and Climate Department, NILU\"\n",
    "#     elif attr =='contributor_name': \n",
    "#         g.add( ( ds, FOAF.name, Literal(dataset.getncattr(attr))))#Markus Fiebig\n",
    "#     elif attr =='contributor_role': \n",
    "#         g.add( ( ds, ae.contributor_role, Literal(dataset.getncattr(attr))))#data submitter\n",
    "#     elif attr =='publisher_type': \n",
    "#         g.add( ( ds, ae.publisher_type, Literal(dataset.getncattr(attr))))#institution\n",
    "#     elif attr =='publisher_name': \n",
    "#         g.add( ( ds, ae.publisher_name, Literal(dataset.getncattr(attr))))#NILU - Norwegian Institute for Air Research, ATMOS, EBAS\n",
    "#     elif attr =='publisher_institution':\n",
    "#         g.add( ( ds, ae.publisher_institution, Literal(dataset.getncattr(attr))))#NILU - Norwegian Institute for Air Research, ATMOS, EBAS\n",
    "#     elif attr =='publisher_email': \n",
    "#         g.add( ( ds, FOAF.mbox, URIRef(dataset.getncattr(attr))))#ebas@nilu.no\n",
    "#     elif attr =='publisher_url': \n",
    "#         g.add( ( ds, ae.publisher_url, Literal(dataset.getncattr(attr))))#http://ebas.nilu.no/\n",
    "#     elif attr =='geospatial_bounds': \n",
    "#         g.add( ( ds, ae.geospatial_bounds, Literal(dataset.getncattr(attr))))#POINT Z (-72.016667 2.533333 1309.0)\n",
    "#     elif attr =='geospatial_bounds_crs': \n",
    "#         g.add( ( ds, ae.geospatial_bounds_crs, Literal(dataset.getncattr(attr))))#EPSG:4979\n",
    "#     elif attr =='geospatial_lat_min':\n",
    "#         g.add( ( ds, ae.geospatial_lat_min, Literal(dataset.getncattr(attr))))#-72.016667\n",
    "#     elif attr =='geospatial_lat_max': \n",
    "#         g.add( ( ds, ae.geospatial_lat_max, Literal(dataset.getncattr(attr))))#-72.016667\n",
    "#     elif attr =='geospatial_lon_min': \n",
    "#         g.add( ( ds, ae.geospatial_lon_min, Literal(dataset.getncattr(attr))))#2.533333\n",
    "#     elif attr =='geospatial_lon_max': \n",
    "#         g.add( ( ds, ae.geospatial_lon_max, Literal(dataset.getncattr(attr))))#2.533333\n",
    "#     elif attr =='geospatial_vertical_min': \n",
    "#         g.add( ( ds, ae.geospatial_vertical_min, Literal(dataset.getncattr(attr))))#1309.0\n",
    "#     elif attr =='geospatial_vertical_max': \n",
    "#         g.add( ( ds, ae.geospatial_vertical_max, Literal(dataset.getncattr(attr))))#1309.0\n",
    "#     elif attr =='geospatial_vertical_positive': \n",
    "#         g.add( ( ds, ae.geospatial_vertical_positive, Literal(dataset.getncattr(attr))))#up\n",
    "#     elif attr =='time_coverage_start': \n",
    "#         g.add( ( ds, ae.time_converage_start, Literal(dataset.getncattr(attr))))#2007-02-18T00:00:00 UTC\n",
    "#     elif attr =='time_coverage_end': \n",
    "#         g.add( ( ds, ae.time_converage_end, Literal(dataset.getncattr(attr))))#2010-01-01T00:00:00 UTC\n",
    "#     elif attr =='time_coverage_duration': \n",
    "#         g.add( ( ds, ae.time_coverage_duration, Literal(dataset.getncattr(attr))))#P0002-10-14T00:00:00\n",
    "#     elif attr =='time_coverage_resolution': \n",
    "#         g.add( ( ds, ae.time_coverage_resolution, Literal(dataset.getncattr(attr))))#P0000-00-00T01:00:00\n",
    "#     elif attr =='timezone': \n",
    "#         g.add( ( ds, ae.timezone, Literal(dataset.getncattr(attr))))#UTC\n",
    "\n",
    "# Iterate over triples in store and print them out.\n",
    "# print(\"\\n--- printing raw triples ---\")\n",
    "# for s, p, o in g:\n",
    "#     print('\\n')\n",
    "#     print((s, p, o))\n",
    "    \n",
    "\n",
    "# print(\"\\n graph has %s statements.\" % len(g))\n",
    "\n",
    "# g.serialize(destination='data.ttl', format='ttl')\n",
    "\n",
    "\n",
    "# print( g.serialize(format='ttl') )\n",
    "\n",
    "##################\n",
    "## Nephelometer ##\n",
    "##################\n",
    "\n",
    "# print('\\nInstrument Nephelometer and variable aerosol scattering coefficient:\\nFilename: https://thredds.nilu.no/thredds/dodsC/ebas/DE0043G.20060101000000.20160708144500.nephelometer..aerosol.4y.1h.DE09L_tsi_neph_3563_aerosol.DE09L_nephelometer.lev2.nc')\n",
    "\n",
    "# # use the netcdf4 lib to extract netcdf via the opendap protocol\n",
    "# dataset = netCDF4.Dataset('DE0043G.20100101000000.20181114115100.cpc.particle_number_concentration.pm10.1y.1h.DE09L_TSI_CPC_3025.DE09L_cpc.lev2.nc','r','NETCDF4')\n",
    "\n",
    "# # fetch global attributes from netcdf on the thredds server\n",
    "# global_attr = dataset.ncattrs()\n",
    "\n",
    "\n",
    "# for attr in global_attr:\n",
    "#     #print('{0}: {1}'.format(attr, dataset.getncattr(attr)))\n",
    "#     if attr==\"summary\":\n",
    "#         print(attr)\n",
    "#         print(dataset.getncattr(attr))\n",
    "#         sentence = dataset.getncattr(attr)\n",
    "#         tokens = nltk.word_tokenize(sentence)\n",
    "#         tagged = nltk.pos_tag(tokens)\n",
    "#         print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
